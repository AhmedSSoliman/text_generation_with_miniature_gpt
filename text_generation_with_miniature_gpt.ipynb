{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_generation_with_miniature_gpt.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxXy7O2MTpzK"
      },
      "source": [
        "# Text generation with a miniature GPT\n",
        "\n",
        "\n",
        "**Description:** Implement a miniature version of GPT and train it to generate text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0uxS2SjTpzY"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This example demonstrates how to implement an autoregressive language model\n",
        "using a miniature version of the GPT model.\n",
        "The model consists of a single Transformer block with causal masking\n",
        "in its attention layer.\n",
        "We use the text from the IMDB sentiment classification dataset for training\n",
        "and generate new movie reviews for a given prompt.\n",
        "When using this script with your own dataset, make sure it has at least\n",
        "1 million words.\n",
        "\n",
        "This example should be run with `tf-nightly>=2.3.0-dev20200531` or\n",
        "with TensorFlow 2.3 or higher.\n",
        "\n",
        "**References:**\n",
        "\n",
        "- [GPT](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n",
        "- [GPT-2](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)\n",
        "- [GPT-3](https://arxiv.org/abs/2005.14165)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrDUphCfTpza"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6zlewdVTpzb"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import random\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxAPh3rJTpzd"
      },
      "source": [
        "## Implement a Transformer block as a layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vSWHlxNTpzf"
      },
      "source": [
        "\n",
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    \"\"\"\n",
        "    Mask the upper half of the dot product matrix in self attention.\n",
        "    This prevents flow of information from future tokens to current token.\n",
        "    1's in the lower triangle, counting from the lower right corner.\n",
        "    \"\"\"\n",
        "    i = tf.range(n_dest)[:, None]\n",
        "    j = tf.range(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    mask = tf.cast(m, dtype)\n",
        "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "    mult = tf.concat(\n",
        "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
        "    )\n",
        "    return tf.tile(mask, mult)\n",
        "\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
        "        attention_output = self.dropout1(attention_output)\n",
        "        out1 = self.layernorm1(inputs + attention_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEagqoW6Tpzh"
      },
      "source": [
        "## Implement an embedding layer\n",
        "\n",
        "Create two seperate embedding layers: one for tokens and one for token index\n",
        "(positions)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oldReEvLTpzi"
      },
      "source": [
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--a1YOeGTpzj"
      },
      "source": [
        "## Implement the miniature GPT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKt-Dr68Tpzl"
      },
      "source": [
        "vocab_size = 20000  # Only consider the top 20k words\n",
        "maxlen = 80  # Max sequence size\n",
        "embed_dim = 256  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
        "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "    x = embedding_layer(inputs)\n",
        "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
        "    x = transformer_block(x)\n",
        "    outputs = layers.Dense(vocab_size)(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    model.compile(\n",
        "        \"adam\", loss=[loss_fn, None],\n",
        "    )  # No loss and optimization based on word embeddings from transformer block\n",
        "    return model\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMtS0o77Tpzn"
      },
      "source": [
        "## Prepare the data for word-level language modelling\n",
        "\n",
        "Download the IMDB dataset and combine training and validation sets for a text\n",
        "generation task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EronRulgTpzp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63cee2c8-6eca-4507-a686-ffb52ec40215"
      },
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  21.3M      0  0:00:03  0:00:03 --:--:-- 21.3M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcBan2DyTpzp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f03f52d-f655-4c50-c3cc-15ac1cfb2620"
      },
      "source": [
        "\n",
        "batch_size = 128\n",
        "\n",
        "# The dataset contains each review in a separate text file\n",
        "# The text files are present in four different folders\n",
        "# Create a list all files\n",
        "filenames = []\n",
        "directories = [\n",
        "    \"aclImdb/train/pos\",\n",
        "    \"aclImdb/train/neg\",\n",
        "    \"aclImdb/test/pos\",\n",
        "    \"aclImdb/test/neg\",\n",
        "]\n",
        "for dir in directories:\n",
        "    for f in os.listdir(dir):\n",
        "        filenames.append(os.path.join(dir, f))\n",
        "\n",
        "print(f\"{len(filenames)} files\")\n",
        "\n",
        "# Create a dataset from text files\n",
        "random.shuffle(filenames)\n",
        "text_ds = tf.data.TextLineDataset(filenames)\n",
        "text_ds = text_ds.shuffle(buffer_size=256)\n",
        "text_ds = text_ds.batch(batch_size)\n",
        "\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n",
        "    lowercased = tf.strings.lower(input_string)\n",
        "    stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
        "\n",
        "\n",
        "# Create a vectorization layer and adapt it to the text\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size - 1,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=maxlen + 1,\n",
        ")\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n",
        "\n",
        "\n",
        "def prepare_lm_inputs_labels(text):\n",
        "    \"\"\"\n",
        "    Shift word sequences by 1 position so that the target for position (i) is\n",
        "    word at position (i+1). The model will use all words up till position (i)\n",
        "    to predict the next word.\n",
        "    \"\"\"\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "text_ds = text_ds.map(prepare_lm_inputs_labels)\n",
        "text_ds = text_ds.prefetch(tf.data.experimental.AUTOTUNE)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQM7PZTBTpzp"
      },
      "source": [
        "## Implement a Keras callback for generating text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAtpXjkETpzp"
      },
      "source": [
        "\n",
        "class TextGenerator(keras.callbacks.Callback):\n",
        "    \"\"\"A callback to generate text from a trained model.\n",
        "    1. Feed some starting prompt to the model\n",
        "    2. Predict probabilities for the next token\n",
        "    3. Sample the next token and add it to the next input\n",
        "\n",
        "    Arguments:\n",
        "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
        "        start_tokens: List of integers, the token indices for the starting prompt.\n",
        "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
        "        top_k: Integer, sample from the `top_k` token predictions.\n",
        "        print_every: Integer, print after this many epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n",
        "    ):\n",
        "        self.max_tokens = max_tokens\n",
        "        self.start_tokens = start_tokens\n",
        "        self.index_to_word = index_to_word\n",
        "        self.print_every = print_every\n",
        "        self.k = top_k\n",
        "\n",
        "    def sample_from(self, logits):\n",
        "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
        "        indices = np.asarray(indices).astype(\"int32\")\n",
        "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
        "        preds = np.asarray(preds).astype(\"float32\")\n",
        "        return np.random.choice(indices, p=preds)\n",
        "\n",
        "    def detokenize(self, number):\n",
        "        return self.index_to_word[number]\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        start_tokens = [_ for _ in self.start_tokens]\n",
        "        if (epoch + 1) % self.print_every != 0:\n",
        "            return\n",
        "        num_tokens_generated = 0\n",
        "        tokens_generated = []\n",
        "        while num_tokens_generated <= self.max_tokens:\n",
        "            pad_len = maxlen - len(start_tokens)\n",
        "            sample_index = len(start_tokens) - 1\n",
        "            if pad_len < 0:\n",
        "                x = start_tokens[:maxlen]\n",
        "                sample_index = maxlen - 1\n",
        "            elif pad_len > 0:\n",
        "                x = start_tokens + [0] * pad_len\n",
        "            else:\n",
        "                x = start_tokens\n",
        "            x = np.array([x])\n",
        "            y, _ = self.model.predict(x)\n",
        "            sample_token = self.sample_from(y[0][sample_index])\n",
        "            tokens_generated.append(sample_token)\n",
        "            start_tokens.append(sample_token)\n",
        "            num_tokens_generated = len(tokens_generated)\n",
        "        txt = \" \".join(\n",
        "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
        "        )\n",
        "        print(f\"generated text:\\n{txt}\\n\")\n",
        "\n",
        "\n",
        "# Tokenize starting prompt\n",
        "word_to_index = {}\n",
        "for index, word in enumerate(vocab):\n",
        "    word_to_index[word] = index\n",
        "\n",
        "start_prompt = \"this movie is\"\n",
        "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
        "num_tokens_generated = 40\n",
        "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfRhyd8zTpzq"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Note: This code should preferably be run on GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVYuRyewTpzq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5158adfa-899d-4afa-d531-54024c89f228"
      },
      "source": [
        "model = create_model()\n",
        "\n",
        "model.fit(text_ds, verbose=2, epochs=25, callbacks=[text_gen_callback])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "391/391 - 80s - loss: 5.5861 - dense_2_loss: 5.5861\n",
            "generated text:\n",
            "this movie is a bad movie . i 'm not that a bad movie i 've never had no idea that it was the film that 's about a little too . i was a little more than that i 've seen the original\n",
            "\n",
            "Epoch 2/25\n",
            "391/391 - 82s - loss: 4.7116 - dense_2_loss: 4.7116\n",
            "generated text:\n",
            "this movie is a [UNK] [UNK] , \" , is a masterpiece of a lot of talent , and i don 't think that this movie should be . it 's about it . i 'm not a very disappointed , i don 't\n",
            "\n",
            "Epoch 3/25\n",
            "391/391 - 81s - loss: 4.4613 - dense_2_loss: 4.4613\n",
            "generated text:\n",
            "this movie is horrible ! the worst of it . it 's an entertaining action movie that makes you want to see this movie . i think it would be a good movie . the acting is great , the plot was bad and\n",
            "\n",
            "Epoch 4/25\n",
            "391/391 - 81s - loss: 4.3036 - dense_2_loss: 4.3036\n",
            "generated text:\n",
            "this movie is an interesting story that is about a young [UNK] \" [UNK] . the story takes place on one [UNK] , the [UNK] of the young [UNK] and a young [UNK] ) is a bit of a man who has to become\n",
            "\n",
            "Epoch 5/25\n",
            "391/391 - 81s - loss: 4.1837 - dense_2_loss: 4.1837\n",
            "generated text:\n",
            "this movie is not a good story about all the acting is very good , but it has some great acting , but the story is great but not good enough to say it 's the best . the plot is so bad that\n",
            "\n",
            "Epoch 6/25\n",
            "391/391 - 81s - loss: 4.0841 - dense_2_loss: 4.0841\n",
            "generated text:\n",
            "this movie is based on the true story that doesn 't matter because of your mind as well known . it 's hard to follow a classic horror anthology series of characters that were underdeveloped and the plot were not so bad , it\n",
            "\n",
            "Epoch 7/25\n",
            "391/391 - 81s - loss: 3.9987 - dense_2_loss: 3.9987\n",
            "generated text:\n",
            "this movie is not an excellent movie , but i 'm glad to get to it . it 's got a lot more than the other people in a way . but when i first saw it , i decided to buy it .\n",
            "\n",
            "Epoch 8/25\n",
            "391/391 - 81s - loss: 3.9246 - dense_2_loss: 3.9246\n",
            "generated text:\n",
            "this movie is so funny and the movie are so lame , you cant even believe that the actors . they 're actually good . . . the plot is not good enough that you just have to make you laugh when you have\n",
            "\n",
            "Epoch 9/25\n",
            "391/391 - 81s - loss: 3.8589 - dense_2_loss: 3.8589\n",
            "generated text:\n",
            "this movie is a good movie , but is a lot . the story is not the acting . it is a lot of great action , and great action scenes with good acting from good and a fine cast of good actors ,\n",
            "\n",
            "Epoch 10/25\n",
            "391/391 - 81s - loss: 3.7998 - dense_2_loss: 3.7998\n",
            "generated text:\n",
            "this movie is a very funny movie . it has great visuals with lots of good action scenes , and gore , explosions and all the characters are great and good . it was great . the plot has been so much more interesting\n",
            "\n",
            "Epoch 11/25\n",
            "391/391 - 81s - loss: 3.7457 - dense_2_loss: 3.7457\n",
            "generated text:\n",
            "this movie is not really a bad idea . the first time of all the [UNK] ' lives of a [UNK] ' , but this is one of them , and a very boring film . if you 're looking for it , not\n",
            "\n",
            "Epoch 12/25\n",
            "391/391 - 81s - loss: 3.6976 - dense_2_loss: 3.6976\n",
            "generated text:\n",
            "this movie is bad , bad and bad acting . bad directing , bad writing , awful directing , bad directing . i don 't have a special mention this is that the plot is bad . . . the acting is horrible ,\n",
            "\n",
            "Epoch 13/25\n",
            "391/391 - 81s - loss: 3.6535 - dense_2_loss: 3.6535\n",
            "generated text:\n",
            "this movie is a very good film . i don 't know what to say ? i think it 's not that good or the bad guy . this is just a movie where you 'll find yourself laughing and get me and the\n",
            "\n",
            "Epoch 14/25\n",
            "391/391 - 82s - loss: 3.6129 - dense_2_loss: 3.6129\n",
            "generated text:\n",
            "this movie is about the same guy who was responsible for a terrible script . the acting is bad , the effects , the plot is horrible , the story is terrible . . it is horrible , it 's not even bad enough\n",
            "\n",
            "Epoch 15/25\n",
            "391/391 - 81s - loss: 3.5759 - dense_2_loss: 3.5759\n",
            "generated text:\n",
            "this movie is a great example of how a bad it is to be . a bad idea , but what is the movie , the worst movie ever made . the acting is horrible , not bad . you can do it ,\n",
            "\n",
            "Epoch 16/25\n",
            "391/391 - 81s - loss: 3.5419 - dense_2_loss: 3.5419\n",
            "generated text:\n",
            "this movie is not bad at all . the acting is atrocious . i mean the plot is just plain dumb . the acting is stupid , bad , bad , bad casting , horrible . it is bad enough , bad acting ,\n",
            "\n",
            "Epoch 17/25\n",
            "391/391 - 81s - loss: 3.5104 - dense_2_loss: 3.5104\n",
            "generated text:\n",
            "this movie is the worst movie i have ever seen ! ! it 's about 5 minutes and i have to say that it 's not . i don 't think you can be laughing so hard you . there is one thing that\n",
            "\n",
            "Epoch 18/25\n",
            "391/391 - 81s - loss: 3.4806 - dense_2_loss: 3.4806\n",
            "generated text:\n",
            "this movie is a classic and i have to say it 's not as good . a very good story line , though it may be a little bit more interesting . the story starts with an interesting premise but then moves . the\n",
            "\n",
            "Epoch 19/25\n",
            "391/391 - 81s - loss: 3.4535 - dense_2_loss: 3.4535\n",
            "generated text:\n",
            "this movie is an incredibly boring movie . it 's so predictable that i 'm not a bad movie . the main character of this movie is supposed to be a bad movie in the same movie . there 's a few jokes here\n",
            "\n",
            "Epoch 20/25\n",
            "391/391 - 81s - loss: 3.4277 - dense_2_loss: 3.4277\n",
            "generated text:\n",
            "this movie is a bad example of how the actors are . the movie is about an italian horror movie . i was not expecting anything good . but it was so bad i was pleasantly surprised . you get to see what the\n",
            "\n",
            "Epoch 21/25\n",
            "391/391 - 81s - loss: 3.4046 - dense_2_loss: 3.4046\n",
            "generated text:\n",
            "this movie is not a masterpiece by any means . but it is a great story about a girl whose husband is dying from cancer . she finds herself to get through her eyes she insists she becomes horribly cruel , that her sister\n",
            "\n",
            "Epoch 22/25\n",
            "391/391 - 81s - loss: 3.3819 - dense_2_loss: 3.3819\n",
            "generated text:\n",
            "this movie is about a group of people , who are not interested in a movie , but they do not know what they are going to do with them to make this one , and a lot of sense of humor and there\n",
            "\n",
            "Epoch 23/25\n",
            "391/391 - 81s - loss: 3.3603 - dense_2_loss: 3.3603\n",
            "generated text:\n",
            "this movie is one of those [UNK] , so i 've ever seen that . and that 's what a bad thing is about it . the only thing in it . . it is that is the only redeeming feature film itself is\n",
            "\n",
            "Epoch 24/25\n",
            "391/391 - 81s - loss: 3.3411 - dense_2_loss: 3.3411\n",
            "generated text:\n",
            "this movie is so bad . the plot is so weak i could barely watch it again . but it 's not worth a watch , i 'll laugh . the movie was so weak . i can 't believe that , i can\n",
            "\n",
            "Epoch 25/25\n",
            "391/391 - 81s - loss: 3.3224 - dense_2_loss: 3.3224\n",
            "generated text:\n",
            "this movie is a classic , so the acting is superb and the directing is outstanding . the story is also very good , the film is a must see it as good in [UNK] as the leading role . [UNK] [UNK] , the\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff480d99b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}